{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d64ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "# -------------------------\n",
    "# Utility conv layers\n",
    "# -------------------------\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Stochastic Basic Block\n",
    "# -------------------------\n",
    "class StochasticBasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic ResNet block with stochastic depth.\n",
    "    - p_survive: survival probability p_l for this block.\n",
    "    Behavior:\n",
    "      - train(): sample b ~ Bernoulli(p_survive).\n",
    "          if b==1: out = x + f(x)\n",
    "          if b==0: out = x  (skip f(x) entirely)\n",
    "      - eval(): deterministic: out = x + p_survive * f(x)\n",
    "        (matches Eq. (5) of the paper).\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, p_survive: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        # survival probability for this block (float between 0 and 1)\n",
    "        self.p_survive = float(p_survive)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # compute residual function f(x) (we will sometimes skip using it)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.training:\n",
    "            # sample once per forward pass (per mini-batch)\n",
    "            rand = torch.rand(1, device=out.device).item()\n",
    "            if rand < self.p_survive:\n",
    "                # block is active\n",
    "                out = identity + out\n",
    "                out = self.relu(out)\n",
    "            else:\n",
    "                # block is bypassed -> identity path only\n",
    "                out = identity\n",
    "        else:\n",
    "            # evaluation: deterministic, scale residual by p_survive (paper Eq. (5))\n",
    "            out = identity + self.p_survive * out\n",
    "            out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Stochastic ResNet\n",
    "# -------------------------\n",
    "class StochasticResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A lightweight ResNet-like model using StochasticBasicBlock.\n",
    "    layers: list with number of blocks in each stage, e.g. [n1, n2, n3]\n",
    "    p_L: final survival probability for the last block; p0 is implicitly 1.0\n",
    "    The code computes a linear-decay p_l per block (see paper Eq. (4)).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers: List[int], num_classes=10, p_L=0.5):\n",
    "        super().__init__()\n",
    "        self.inplanes = 16  # typical for CIFAR-style experiments\n",
    "        self.conv1 = conv3x3(3, 16)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # total number of residual blocks L\n",
    "        self.L = sum(layers)\n",
    "\n",
    "        # create list of survival probabilities for each block (linear decay)\n",
    "        # p_0 is for the \"input\" and is 1; for blocks we compute p_l following Eq (4)\n",
    "        self.p_list = self._compute_survival_probabilities(p_L)\n",
    "\n",
    "        # Build layers (three stages commonly)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0], stride=1, start_block_idx=0)\n",
    "        idx = layers[0]\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2, start_block_idx=idx)\n",
    "        idx += layers[1]\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2, start_block_idx=idx)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _compute_survival_probabilities(self, p_L: float) -> List[float]:\n",
    "        # p_l = 1 - (l / L) * (1 - p_L), l in [1..L]\n",
    "        # We return a list of length L where index l-1 corresponds to p_l.\n",
    "        L = max(1, self.L)\n",
    "        p_list = [1.0 - (float(l) / L) * (1.0 - float(p_L)) for l in range(1, L+1)]\n",
    "        return p_list\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, start_block_idx=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            # downsample using 1x1 conv (common for CIFAR variants they use different trick; this is standard)\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        for i in range(blocks):\n",
    "            # survival prob for this block (global index = start_block_idx + i)\n",
    "            p = self.p_list[start_block_idx + i]\n",
    "            if i == 0:\n",
    "                layers.append(block(self.inplanes, planes, stride, downsample, p_survive=p))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes, 1, None, p_survive=p))\n",
    "            self.inplanes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)   # CIFAR style first conv\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper to construct a CIFAR-ResNet (e.g., 110-layer)\n",
    "# Example: for the ResNet-110 in the paper they use 3 groups each with 18 blocks\n",
    "# (which corresponds to 54 residual blocks). Here layers=[18,18,18]\n",
    "# -------------------------\n",
    "def stochastic_resnet_cifar(layers=[18, 18, 18], num_classes=10, p_L=0.5):\n",
    "    return StochasticResNet(StochasticBasicBlock, layers, num_classes=num_classes, p_L=p_L)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage (training skeleton)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # quick sanity check\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = stochastic_resnet_cifar(layers=[3,3,3], num_classes=10, p_L=0.5).to(device)\n",
    "    # small network: 3 blocks per stage (for quick runs)\n",
    "    x = torch.randn(8, 3, 32, 32).to(device)\n",
    "    model.train()\n",
    "    out = model(x)\n",
    "    print(\"train out shape:\", out.shape)\n",
    "    model.eval()\n",
    "    out_eval = model(x)\n",
    "    print(\"eval out shape:\", out_eval.shape)\n",
    "\n",
    "    # Training skeleton (very short)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # for epoch in range(epochs):\n",
    "    #     model.train()\n",
    "    #     for images, labels in train_loader:\n",
    "    #         images, labels = images.to(device), labels.to(device)\n",
    "    #         optimizer.zero_grad()\n",
    "    #         logits = model(images)\n",
    "    #         loss = criterion(logits, labels)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #     # validation\n",
    "    #     model.eval()\n",
    "    #     # run validation loop...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71788401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from stochastic_resnet import stochastic_resnet_cifar   # <- import the previous model\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Configuration (matching Huang et al. 2016)\n",
    "# ============================================================\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 128\n",
    "INITIAL_LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "P_L = 0.5    # final survival probability (paper default)\n",
    "VALIDATION_SIZE = 5000\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Prepare CIFAR-10 Data\n",
    "# Default augmentation: random crop 32×32 (padding=4), horizontal flip\n",
    "# Matches paper: Section 4 (CIFAR-10)\n",
    "# ============================================================\n",
    "\n",
    "def prepare_data():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "\n",
    "    full_train = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform_train\n",
    "    )\n",
    "\n",
    "    # Split into train/val (45k / 5k)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_train, [len(full_train) - VALIDATION_SIZE, VALIDATION_SIZE]\n",
    "    )\n",
    "    val_dataset.dataset.transform = transform_test  # no augmentation in validation\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_test\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Learning Rate Schedule (Matching the paper)\n",
    "#  - 0.1 initially\n",
    "#  - divide by 10 at epoch 250\n",
    "#  - divide by 10 at epoch 375\n",
    "# ============================================================\n",
    "\n",
    "def lr_schedule(optimizer, epoch):\n",
    "    if epoch == 250:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "    if epoch == 375:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training loop\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Validation loop\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Load data\n",
    "    train_loader, val_loader, test_loader = prepare_data()\n",
    "\n",
    "    # Create ResNet-110 with stochastic depth (3×18 blocks)\n",
    "    model = stochastic_resnet_cifar(layers=[18, 18, 18], num_classes=NUM_CLASSES, p_L=P_L)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss + optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=INITIAL_LR,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        nesterov=True,\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        lr_schedule(optimizer, epoch)\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n",
    "              f\"LR={optimizer.param_groups[0]['lr']:.4f} \"\n",
    "              f\"Train Loss={train_loss:.4f} Acc={train_acc:.4f} \"\n",
    "              f\"Val Loss={val_loss:.4f} Acc={val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_stochastic_resnet_cifar10.pth\")\n",
    "\n",
    "    # Total training time\n",
    "    total_time = (time.time() - start_time) / 3600\n",
    "    print(f\"\\nTraining complete in {total_time:.2f} hours.\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.load_state_dict(torch.load(\"best_stochastic_resnet_cifar10.pth\"))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
