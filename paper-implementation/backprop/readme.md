# Efficient BackProp â€” PyTorch Implementation & Visualizations

This project is a complete, practical implementation of the classic paper â€œEfficient BackPropâ€ â€” LeCun, Bottou, Orr, MÃ¼ller (1998) featuring:

- PyTorch implementation of all recommended optimization tricks
- 2D & 3D loss-surface visualizations
- PCA weight-space trajectory plots
- Hessian diagnostics (HÂ·v products + power method eigenvalue estimation)
- Experiments demonstrating each principle from the paper
- Clear explanations to help you master the underlying concepts

This repo is designed for both learning and research experimentation.

## ğŸš€ What You Will Learn

This project gives you practical intuition and mastery over:

- âœ… Backpropagation and the core update rule: $$\Delta w_i = -\eta x_i \delta$$
- âœ… Why input normalization accelerates convergence
- âœ… Why LeCunâ€™s recommended activation outperforms sigmoids
- âœ… Why weight initialization must follow: $$\sigma_w = \frac{1}{\sqrt{\text{fan-in}}}$$
- âœ… Why SGD often outperforms batch gradient descent
- âœ… How momentum reduces zig-zag in steep valleys
- âœ… What the Hessian is and how curvature affects learning
- âœ… How to compute Hessianâ€“vector products (HÂ·v) in PyTorch
- âœ… How to estimate the top Hessian eigenvalue via the power method
- âœ… How to visualize high-dimensional optimization with PCA
- âœ… How to create 2D & 3D loss landscapes of a neural network

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ efficient_backprop_pytorch.py      # Core implementation and visualization utilities
â”œâ”€â”€ README.md                          # You are here
â”œâ”€â”€ data/                              # Synthetic datasets (2D Gaussians)
â”œâ”€â”€ experiments/                       # Optional: saved figures & logs
â””â”€â”€ notebooks/
    â”œâ”€â”€ EfficientBackProp_Full.ipynb   # Full tutorial notebook (all experiments)
    â””â”€â”€ Hessian_DeepDive.ipynb         # Notebook focused on Hessian & curvature
```

## ğŸ§  Techniques Implemented (Directly from the Paper)

1. **Input Standardization & Whitening**
   - Shift inputs to zero mean
   - Scale to unit variance
   - Optional PCA whitening
   - Improves Hessian conditioning and speeds up convergence.

2. **LeCunâ€™s Recommended Activation**
   - $$1.7159 \times \tanh\left(\frac{2}{3} \times x\right)$$
   - Chosen to avoid saturation and maintain stable gradient flow.

3. **LeCun Weight Initialization**
   - $$\text{std} = \frac{1}{\sqrt{\text{fan-in}}}$$
   - Keeps activations within a useful range at the start of training.

4. **SGD / Mini-batch Training with Momentum**
   - Faster convergence
   - Better minima
   - Less zig-zag behavior
   - Exactly as recommended in Efficient BackProp.

5. **Hessian Diagnostics**
   - Compute Hessianâ€“vector products (HÂ·v)
   - Approximate largest eigenvalue
   - Track curvature during training

6. **2D & 3D Loss-Surface Visualization**
   - Fix all parameters except two
   - Plot contour & 3D surface
   - Overlay training trajectory

7. **PCA Visualization of Weight Trajectories**
   - Compress entire training path into 2D/3D for intuitive understanding.

## ğŸ“Š Visualizations Included

- âœ… Training loss (log-scale)
- âœ… PCA trajectory (2D & 3D)
- âœ… Loss-surface contour + trajectory overlay
- âœ… 3D loss surface
- âœ… Hessian top eigenvalue over training
- âœ… Comparison plots for different initializations, activations & preprocessing

## â–¶ï¸ How to Run

1. Install dependencies
   ```
   pip install torch numpy matplotlib scikit-learn
   ```

2. Run the main script
   ```
   python efficient_backprop_pytorch.py
   ```

3. Or launch the full notebook
   ```
   jupyter notebook notebooks/EfficientBackProp_Full.ipynb
   ```

## ğŸ§ª Experiments You Can Reproduce

Each experiment is directly tied to a specific section of the paper.

1. **Baseline (bad preprocessing)**
   - Logistic sigmoid
   - Poor initialization
   - No normalization
   - Batch gradient
   - Expect: slow convergence, zig-zag trajectories, large eigenvalue spread.

2. **Add Input Normalization**
   - Expect: faster convergence, improved conditioning, smoother trajectories.

3. **Use Recommended Activation (LeCunTanh)**
   - Expect: less saturation, better gradient flow.

4. **Use Proper LeCun Initialization**
   - Expect: stable activations, faster initial descent.

5. **SGD vs Batch Gradient**
   - SGD tends to:
     - Converge faster
     - Escape shallow minima
     - Reach better solutions

6. **Momentum**
   - Expect: reduced oscillation in steep directions.

7. **PCA Weight Trajectories**
   - Visualize how optimization moves through weight space.

8. **Hessian Eigenvalue Analysis**
   - Track top eigenvalue across epochs to see conditioning change.

## ğŸ¯ Goal of this Project

By completing the experiments and visualizations in this repository, you will develop:

- Intuition for why deep learning optimization is hard
- Understanding of conditioning, curvature, and the Hessian
- Practical mastery of initialization & preprocessing techniques
- Geometric insights into gradient descent dynamics

All grounded in one of the most important optimization papers ever written.

## ğŸ“˜ References

LeCun, Yann; Bottou, LÃ©on; Orr, Genevieve; MÃ¼ller, Klaus.  
*Efficient BackProp* (1998).
