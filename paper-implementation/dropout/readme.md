ğŸ“˜ Dropout Experiment â€” Reproducing Srivastava et al. (2014) with MNIST & CIFAR-10

This project implements convolutional neural networks (CNNs) using Dropout, following the classic paper:
â€œDropout: A Simple Way to Prevent Neural Networks from Overfittingâ€ â€” Srivastava et al., 2014
Local paper reference: /mnt/data/srivastava14a.pdf

The goal is to understand dropout through:

A faithful reproduction of the paperâ€™s MNIST architecture

Modern PyTorch implementations

Visual explanations (2D, interactive 3D plots)

Training metrics and comparisons

Optional CIFAR-10 experiments with dropout

ğŸš€ What is Dropout? (Simple Explanation)

Dropout is a regularization technique that prevents overfitting by randomly turning off neurons during training.
This forces the network to learn robust, independent features instead of depending on specific neurons.

Real-life analogy

If you study with different friends absent each day, you become stronger in every subject because you canâ€™t depend on just one expert.
Thatâ€™s exactly what dropout does for neural networks.

ğŸ“‚ What This Project Includes
âœ”ï¸ 1. MNIST CNN (as in the original paper)

Architecture:

Conv5x5 â†’ ReLU â†’ MaxPool
Conv5x5 â†’ ReLU â†’ MaxPool
Fully Connected 1024
Dropout (p = 0.5)
Output Layer (10 classes)


Hyperparameters follow the paper:

Dropout probability: 0.5 on hidden layers

SGD: lr = 0.1, momentum = 0.95

Max-norm constraint on incoming weights

Batch size: 100â€“256

âœ”ï¸ 2. CIFAR-10 Experiment (optional)

Includes a deeper CNN suitable for CIFAR-10 with dropout applied before fully connected layers.

âœ”ï¸ 3. Training Metrics & Visualization

The notebook records and plots:

Training & test loss curves

Training & test accuracy curves

Plots help visualize the effect of dropout on generalization.

âœ”ï¸ 4. Image Visualizations

2D matplotlib visualizations of MNIST & CIFAR images

Interactive 3D surface plots (Plotly) showing pixel intensities in 3D
Great for building intuition about image data & model inputs.

âœ”ï¸ 5. GPU Support

Automatically uses GPU if available:

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

ğŸ“ Files Included

mnist_dropout_full.ipynb â€” main notebook

mnist_dropout_cnn.ipynb â€” simple version

srivastava14a.pdf â€” uploaded paper

Checkpoints saved during training (optional)

ğŸ§  How to Run
1. Install dependencies
pip install torch torchvision matplotlib plotly tqdm

2. Open the notebook
jupyter notebook mnist_dropout_full.ipynb

3. Run all cells

The notebook will:

Download MNIST

Train the CNN with dropout

Plot accuracy & loss curves

Display 2D & 3D visualizations

(Optional) Train on CIFAR-10

ğŸ“Š Example Results

Typical MNIST results with dropout:

Test accuracy: ~99%

Shows less overfitting compared to networks without dropout

Typical CIFAR-10 results:

Test accuracy: 75â€“82% (small convnet)

ğŸ” Why This Matters

The Dropout paper was a major milestone in deep learning.
It showed that a simple trick â€” randomly dropping neurons â€” acts like training many subnetworks and averaging them, dramatically improving generalization.

This repository helps you see and feel that effect using clean, reproducible experiments.

ğŸ“ Reference

Srivastava, Hinton, Krizhevsky, Sutskever, Salakhutdinov (2014)
â€œDropout: A Simple Way to Prevent Neural Networks from Overfittingâ€
JMLR 15(1):1929-1958
Included locally at: /mnt/data/srivastava14a.pdf