# ğŸ“˜ Layer Normalization Deep Dive â€” README

## ğŸ” A Hands-On Exploration of Layer Normalization

This repository provides a complete practical guide to Layer Normalization (LN) â€” a powerful technique in deep learning that stabilizes and accelerates training, especially in RNNs, Transformers, and small-batch learning environments.

The notebook included here demonstrates LN using:

- ğŸ”¥ Heatmaps of activation correlations
- ğŸŒ Plotly 3D interactive visualizations
- ğŸ“Œ Animated gradient flow through training
- ğŸ§ª Training comparisons: No Norm vs BatchNorm vs LayerNorm
- âš™ï¸ Implementations in MLPs, RNNs, CNNs, Transformer blocks
- ğŸ’¡ Effects of LN initialization (Î³, Î²)

Based on the foundational research paper:

**Layer Normalization**  
Jimmy Lei Ba, Jamie Ryan Kiros & Geoffrey E. Hinton  
University of Toronto & Google  
arXiv:1607.06450 (2016)  
https://arxiv.org/abs/1607.06450

## ğŸ§  Why Layer Normalization?

Batch Normalization normalizes activations across a batch which causes issues:

| Issue              | BatchNorm Problem          |
|--------------------|----------------------------|
| Small batch sizes  | Unstable statistics        |
| Sequence training  | Needs separate stats per timestep |
| Online / RL settings | Batch size = 1 â†’ fails    |
| Inference          | Stats differ from training |

LayerNorm solves this by normalizing across the hidden units within each sample, enabling:

- âœ” Stable & faster RNN training
- âœ” Same behavior in training & inference
- âœ” Strong performance even with tiny batches
- âœ” Robust gradient flow

## ğŸ“‚ Repository Structure

```
ğŸ“ LayerNorm-DeepDive/
â”‚
â”œâ”€â”€ layernorm_visualization.ipynb   # ğŸ”¥ Main interactive notebook
â”œâ”€â”€ README.md                       # ğŸ“˜ This documentation
â””â”€â”€ assets/                         # (Optional) Generated figures / animations
```

## ğŸš€ Features Demonstrated

### ğŸ”¹ 1ï¸âƒ£ Custom LayerNorm Implementation (from Paper Eq. 15â€“16)

- Learnable Î³ (gain) and Î² (bias) per feature
- Stable statistics per-training case

### ğŸ”¹ 2ï¸âƒ£ Visualizations Include

| Visualization              | What It Shows                          |
|----------------------------|----------------------------------------|
| Heatmaps                   | LN reduces hidden unit correlation     |
| 3D Plotly                  | Activations become normalized & spherical |
| Training Loss Animated     | LN speeds convergence                  |
| Gradient Flow Animation    | LN prevents exploding/vanishing gradients |

### ğŸ”¹ 3ï¸âƒ£ Architecture Comparisons

| Model              | Normalization Tested          |
|--------------------|-------------------------------|
| MLP                | Baseline vs LN                |
| RNN (LSTM)         | BatchNorm vs LayerNorm        |
| CNN                | BatchNorm vs LayerNorm        |
| Transformer Block  | BatchNorm vs LayerNorm        |

## ğŸ”§ Installation

You can run locally or in Google Colab.

### Requirements

- python >= 3.8
- torch >= 2.0
- numpy
- matplotlib
- seaborn
- scikit-learn
- plotly

Install via pip:

```bash
pip install torch numpy matplotlib seaborn scikit-learn plotly
```

## â–¶ï¸ Running the Notebook

Launch Jupyter:

```bash
jupyter notebook layernorm_visualization.ipynb
```

Or upload to Google Colab:

ğŸ‘‰ Open Colab: https://colab.research.google.com/

ğŸ“Œ Upload the notebook file

## ğŸ“Š Key Takeaways

- âœ” LN keeps mean â‰ˆ 0 and variance â‰ˆ 1 per sample
- âœ” LN makes networks robust to input scale and weight initialization
- âœ” LN accelerates convergence and improves final accuracy
- âœ” Gradients flow smoother across layers & time steps
- âœ” LN is now standard in Transformers (BERT, GPT, T5â€¦)

## ğŸ“‘ Scientific Notes

- LN is invariant to rescaling a layerâ€™s weights
- LN stabilizes hidden-to-hidden dynamics in recurrent networks
- LN outperforms BN in long sequence tasks & online learning
- BN remains better for CNNs with large batch sizes

Reference Table from paper:

| Model Type                  | Best Normalization |
|-----------------------------|---------------------|
| RNNs / NLP                  | â­ LayerNorm       |
| Transformers                | â­ LayerNorm       |
| CNNs (batch â‰¥ 32)           | ğŸ‘ BatchNorm       |
| Online / Reinforcement Learning | â­ LayerNorm    |

## ğŸ™Œ Acknowledgements

This work builds on:

- ğŸ“„ Ba, Kiros & Hinton â€” Layer Normalization (2016)
- ğŸ” Ioffe & Szegedy â€” Batch Normalization (2015)
