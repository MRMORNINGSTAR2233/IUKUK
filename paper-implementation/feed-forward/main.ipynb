{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea6cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fca4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softsign activation (used in the paper)\n",
    "class Softsign(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x / (1 + torch.abs(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10d531c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def glorot_init(tensor):\n",
    "    if tensor.dim() < 2:\n",
    "        raise ValueError(\"Tensor must have at least 2 dimensions for Glorot init.\")\n",
    "    \n",
    "    fan_in, fan_out = tensor.size(1), tensor.size(0)\n",
    "    bound = math.sqrt(6.0 / (fan_in + fan_out))\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-bound, bound)\n",
    "        \n",
    "linear = nn.Linear(256, 512)\n",
    "glorot_init(linear.weight)\n",
    "nn.init.zeros_(linear.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7284ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (activation): Softsign()\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
      "    (1): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (2): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=\"tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        acts = {\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"softsign\": Softsign(),\n",
    "        }\n",
    "        self.activation = acts[activation]\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i+1]\n",
    "            linear = nn.Linear(in_f, out_f)\n",
    "            glorot_init(linear.weight)     # â† our Xavier initialization\n",
    "            nn.init.zeros_(linear.bias)\n",
    "            layers.append(linear)\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.layers[:-1]:\n",
    "            x = self.activation(linear(x))\n",
    "        return self.layers[-1](x)  # last layer linear\n",
    "\n",
    "model = MLP([784, 1000, 1000, 10], activation=\"softsign\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e77ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_activation_statistics(model, input_dim=784, samples=1000):\n",
    "    x = torch.randn(samples, input_dim)\n",
    "    \n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for layer in model.layers[:-1]:\n",
    "            x = model.activation(layer(x))\n",
    "            activations.append(x.clone())\n",
    "\n",
    "    for i, a in enumerate(activations):\n",
    "        print(f\"Layer {i+1}: mean={a.mean():.4f}, std={a.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63e4445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: mean=0.0007, std=0.6088\n",
      "Layer 2: mean=-0.0001, std=0.4761\n"
     ]
    }
   ],
   "source": [
    "check_activation_statistics(MLP([784, 1000, 1000, 1000], \"tanh\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7265b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient_flow(model, input_dim=784, batch=32):\n",
    "    x = torch.randn(batch, input_dim)\n",
    "    y = torch.randint(0, 10, (batch,))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        grad = layer.weight.grad\n",
    "        print(f\"Layer {i+1} grad std = {grad.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a28c41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 grad std = 0.003128\n",
      "Layer 2 grad std = 0.003035\n",
      "Layer 3 grad std = 0.014662\n"
     ]
    }
   ],
   "source": [
    "check_gradient_flow(MLP([784, 512, 512, 10], \"softsign\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda543e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_jacobian_singular_values(layer, x):\n",
    "    x = x.requires_grad_(True)\n",
    "    y = layer(x)\n",
    "    \n",
    "    jacobian = []\n",
    "    for i in range(y.shape[1]):  \n",
    "        grad_outputs = torch.zeros_like(y)\n",
    "        grad_outputs[:, i] = 1.0\n",
    "        grad = torch.autograd.grad(y, x, grad_outputs=grad_outputs, retain_graph=True)[0]\n",
    "        jacobian.append(grad)\n",
    "    \n",
    "    J = torch.stack(jacobian, dim=1)  # shape: [batch, out, in]\n",
    "    J = J.mean(0)                      # average over batch\n",
    "\n",
    "    # compute singular values\n",
    "    U, S, V = torch.svd(J)\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7688828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average singular value: 0.849372386932373\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(100, 100)\n",
    "glorot_init(layer.weight)\n",
    "\n",
    "x = torch.randn(32, 100)\n",
    "sv = estimate_jacobian_singular_values(layer, x)\n",
    "\n",
    "print(\"Average singular value:\", sv.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347732f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_demo(activation=\"tanh\"):\n",
    "    model = MLP([784, 1000, 1000, 10], activation=activation)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for step in range(200):\n",
    "        x = torch.randn(64, 784)\n",
    "        y = torch.randint(0, 10, (64,))\n",
    "\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(f\"[{activation}] Step {step}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f0c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sigmoid] Step 0, Loss = 2.6037\n",
      "[sigmoid] Step 20, Loss = 2.3526\n",
      "[sigmoid] Step 40, Loss = 2.4421\n",
      "[sigmoid] Step 60, Loss = 2.3401\n",
      "[sigmoid] Step 80, Loss = 2.4809\n",
      "[sigmoid] Step 100, Loss = 2.3195\n",
      "[sigmoid] Step 120, Loss = 2.3395\n",
      "[sigmoid] Step 140, Loss = 2.3540\n",
      "[sigmoid] Step 160, Loss = 2.4798\n",
      "[sigmoid] Step 180, Loss = 2.3993\n",
      "[tanh] Step 0, Loss = 2.4091\n",
      "[tanh] Step 20, Loss = 2.3892\n",
      "[tanh] Step 40, Loss = 2.5889\n",
      "[tanh] Step 60, Loss = 2.6190\n",
      "[tanh] Step 80, Loss = 2.4979\n",
      "[tanh] Step 100, Loss = 2.6531\n",
      "[tanh] Step 120, Loss = 2.3152\n",
      "[tanh] Step 140, Loss = 2.4932\n",
      "[tanh] Step 160, Loss = 2.4560\n",
      "[tanh] Step 180, Loss = 2.4853\n",
      "[softsign] Step 0, Loss = 2.2867\n",
      "[softsign] Step 20, Loss = 2.2622\n",
      "[softsign] Step 40, Loss = 2.4207\n",
      "[softsign] Step 60, Loss = 2.3916\n",
      "[softsign] Step 80, Loss = 2.3379\n",
      "[softsign] Step 100, Loss = 2.3843\n",
      "[softsign] Step 120, Loss = 2.4374\n",
      "[softsign] Step 140, Loss = 2.3562\n",
      "[softsign] Step 160, Loss = 2.3907\n",
      "[softsign] Step 180, Loss = 2.3665\n"
     ]
    }
   ],
   "source": [
    "train_demo(\"sigmoid\")\n",
    "train_demo(\"tanh\")\n",
    "train_demo(\"softsign\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
