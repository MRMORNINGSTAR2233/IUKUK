# Natural Language Processing (NLP) â€” Implementation Checklist

A curated list of foundational + modern NLP papers.

---

## ğŸ”¤ Word Embeddings & Pre-Transformers

- [ ] [Efficient Estimation of Word Representations (word2vec) â€” Mikolov et al., 2013](https://arxiv.org/abs/1310.4546)
- [ ] [GloVe â€” Pennington et al.] (Add link if needed)

---

## ğŸ” Sequence Models & Attention

- [ ] [Sequence to Sequence Learning with Neural Networks â€” Sutskever et al., 2014](https://arxiv.org/abs/1409.3215)
- [ ] [Neural Machine Translation by Joint Learning to Align & Translate (Bahdanau Attention) â€” 2015](https://arxiv.org/abs/1409.0473)
- [ ] [Attention Is All You Need â€” Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)

---

## ğŸ§  Contextual Embeddings & Transfer Learning

- [ ] [ELMo â€” Peters et al., 2018](https://arxiv.org/abs/1802.05365)
- [ ] [ULMFiT â€” Howard & Ruder, 2018](https://arxiv.org/abs/1801.06146)

---

## ğŸ¤– Transformer Encoder Models

- [ ] [BERT â€” Devlin et al., 2018](https://arxiv.org/abs/1810.04805)
- [ ] [RoBERTa â€” Liu et al., 2019](https://arxiv.org/abs/1907.11692)
- [ ] [XLNet â€” Yang et al., 2019](https://arxiv.org/abs/1906.08237)
- [ ] [ALBERT â€” Lan et al., 2019](https://arxiv.org/abs/1909.11942)
- [ ] [SpanBERT â€” Joshi et al., 2020](https://arxiv.org/abs/1907.10529)
- [ ] [DeBERTa â€” He et al., 2021](https://arxiv.org/abs/2006.03654)
- [ ] [Longformer â€” Beltagy et al., 2020](https://arxiv.org/abs/2004.05150)

---

## ğŸ“ Seq2Seq, Summarization & Retrieval

- [ ] [BART â€” Lewis et al., 2019](https://arxiv.org/abs/1910.13461)
- [ ] [T5 â€” Raffel et al., 2020](https://arxiv.org/abs/1910.10683)
- [ ] [DPR â€” Karpukhin et al., 2020](https://arxiv.org/abs/2004.04906)
- [ ] [RAG â€” Lewis et al., 2020](https://arxiv.org/abs/2005.11401)

---

## âš¡ Efficient Transformers & Scaling

- [ ] [ELECTRA â€” Clark et al., 2020](https://arxiv.org/abs/2003.10555)
- [ ] [Switch Transformers â€” Fedus et al., 2021](https://arxiv.org/abs/2101.03961)

---

If you'd like, I can also add:
- ğŸ“Œ Difficulty tags  
- ğŸ“Œ Implementation priority order  
- ğŸ“Œ Links to official code repos  
